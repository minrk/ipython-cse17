{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because what's a parallel computing demo without counting words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utilitiles for excluding commmon phrases and normalizing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "non_word = re.compile(r'[\\W\\d]+', re.UNICODE)\n",
    "\n",
    "def normalize_word(word):\n",
    "    \"\"\"normalize a word\n",
    "    \n",
    "    simply strips non-word characters and case\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    word = non_word.sub('', word)\n",
    "    return word\n",
    "\n",
    "common_words = {\n",
    "'the','of','and','in','to','a','is','it','that','which','as','on','by',\n",
    "'be','this','with','are','from','will','at','you','not','for','no','have',\n",
    "'i','or','if','his','its','they','but','their','one','all','he','when',\n",
    "'than','so','these','them','may','see','other','was','has','an','there',\n",
    "'more','we','footnote', 'who', 'had', 'been',  'she', 'do', 'what',\n",
    "'her', 'him', 'my', 'me', 'would', 'could', 'said', 'am', 'were', 'very',\n",
    "'your', 'did', 'not',\n",
    "}\n",
    "\n",
    "def yield_words(filename):\n",
    "    \"\"\"returns a generator of words in a file\"\"\"\n",
    "    import io\n",
    "    with io.open(filename, errors='replace') as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                word = normalize_word(word)\n",
    "                if word:\n",
    "                    yield word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that reads a file, and returns a dictionary\n",
    "with string keys of phrases of `n` words,\n",
    "whose values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(filename, n=1):\n",
    "    \"\"\"compute ngram counts for the contents of a file\"\"\"\n",
    "    word_iterator = yield_words(filename)\n",
    "    counts = {}\n",
    "    def _count_gram(gram):\n",
    "        common = sum(word in common_words for word in gram)\n",
    "        if common > n / 2.0:\n",
    "            # don't count ngrams that are >= 50% common words\n",
    "            return\n",
    "        sgram = ' '.join(gram)\n",
    "        counts.setdefault(sgram, 0)\n",
    "        counts[sgram] += 1\n",
    "    \n",
    "    gram = []\n",
    "    \n",
    "    # get the first word\n",
    "    while len(gram) < n:\n",
    "        try:\n",
    "            word = next(word_iterator)\n",
    "            if not word:\n",
    "                continue\n",
    "        except StopIteration:\n",
    "            return counts\n",
    "        else:\n",
    "            gram.append(word)\n",
    "    \n",
    "    _count_gram(gram)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            word = next(word_iterator)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        else:\n",
    "            gram.append(word)\n",
    "            gram.pop(0)\n",
    "            _count_gram(gram)\n",
    "    return counts\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cathat.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile cathat.txt\n",
    "the cat in the hat is a cat whose hat is big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'big': 1, 'cat': 2, 'hat': 2, 'whose': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams('cathat.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a cat': 1,\n",
       " 'cat in': 1,\n",
       " 'cat whose': 1,\n",
       " 'hat is': 2,\n",
       " 'is big': 1,\n",
       " 'the cat': 1,\n",
       " 'the hat': 1,\n",
       " 'whose hat': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams('cathat.txt', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fetch some interesting data from Project Gutenberg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    from urllib.request import urlretrieve # py3\n",
    "except ImportError:\n",
    "    from urllib import urlretrieve # py2\n",
    "\n",
    "davinci_url = \"http://www.gutenberg.org/files/5000/5000-8.txt\"\n",
    "\n",
    "if not os.path.exists('davinci.txt'):\n",
    "    # download from project gutenberg\n",
    "    print(\"Downloading Da Vinci's notebooks from Project Gutenberg\")\n",
    "    urlretrieve(davinci_url, 'davinci.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def print_common(freqs, n=10):\n",
    "    \"\"\"Print the n most common keys by count.\"\"\"\n",
    "    words, counts = freqs.keys(), freqs.values()\n",
    "    items = zip(counts, words)\n",
    "    items = sorted(items, reverse=True)\n",
    "    justify = 0\n",
    "    for (count, word) in items[:n]:\n",
    "        justify = max(justify, len(word))\n",
    "    \n",
    "    for (count, word) in items[:n]:\n",
    "        print(word.rjust(justify), count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial word frequency count:\n",
      "CPU times: user 632 ms, sys: 6.38 ms, total: 638 ms\n",
      "Wall time: 638 ms\n",
      "   light 849\n",
      "     eye 589\n",
      "    same 536\n",
      "  shadow 507\n",
      "    body 454\n",
      " between 445\n",
      "   water 425\n",
      "leonardo 416\n",
      "    seen 415\n",
      "    into 403\n"
     ]
    }
   ],
   "source": [
    "# Run the serial version\n",
    "print(\"Serial word frequency count:\")\n",
    "%time counts = ngrams('davinci.txt', 1)\n",
    "print_common(counts, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the davinci.txt into one file per engine:\n",
    "text = open('davinci.txt', encoding='latin1', errors='replace').read()\n",
    "lines = text.splitlines()\n",
    "nlines = len(lines)\n",
    "n = 10\n",
    "\n",
    "block = nlines//n\n",
    "for i in range(n):\n",
    "    chunk = lines[i*block:(i+1)*(block)]\n",
    "    with open('davinci%i.txt' % i, 'w') as f:\n",
    "        f.write('\\n'.join(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "fnames = [ os.path.join(cwd, 'davinci%i.txt' % i) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "rc = ipp.Client()\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult: _push>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = rc.load_balanced_view()\n",
    "eall = rc[:]\n",
    "eall.push(dict(\n",
    "    non_word=non_word,\n",
    "    yield_words=yield_words,\n",
    "    common_words=common_words,\n",
    "    normalize_word=normalize_word,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: parallel ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a version of ngrams that runs in parallel,\n",
    "rejoining the results into a single count dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../soln/ngrams.py\n",
    "def ngrams_parallel(view, fnames, n=1):\n",
    "    \"\"\"Compute ngrams in parallel\n",
    "    \n",
    "    view - An IPython DirectView\n",
    "    fnames - The filenames containing the split data.\n",
    "    \"\"\"\n",
    "\n",
    "    ar = view.map_async(ngrams, fnames, [n] * len(fnames))\n",
    "    counts = {}\n",
    "    for engine_count in ar:\n",
    "        for gram, count in engine_count.items():\n",
    "            if gram not in counts:\n",
    "                counts[gram] = 0\n",
    "            counts[gram] += count\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel ngrams\n",
      "CPU times: user 349 ms, sys: 28.9 ms, total: 378 ms\n",
      "Wall time: 585 ms\n",
      "  light and shade 98\n",
      "     the same way 44\n",
      "the luminous body 33\n",
      "  between the eye 31\n",
      "the space between 29\n",
      "      pen and ink 29\n",
      "leonardo da vinci 29\n",
      "   the solar rays 27\n",
      "   the right hand 27\n",
      "space between the 27\n"
     ]
    }
   ],
   "source": [
    "print(\"Parallel ngrams\")\n",
    "%time pcounts = ngrams_parallel(view, fnames, 3)\n",
    "print_common(pcounts, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some Project Gutenberg samples from ntlk (avoid rate-limiting on PG itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_samples = 'http://nltk.github.com/nltk_data/packages/corpora/gutenberg.zip'\n",
    "if not os.path.isdir('gutenberg'):\n",
    "    if not os.path.exists('gutenberg.zip'):\n",
    "        urlretrieve(gutenberg_samples, 'gutenberg.zip')\n",
    "    !unzip gutenberg.zip\n",
    "\n",
    "import glob\n",
    "gutenberg_files = glob.glob(os.path.abspath(os.path.join('gutenberg', '*.txt')))\n",
    "# remove the bible, because it's too big relative to the rest\n",
    "gutenberg_files.remove(os.path.abspath(os.path.join('gutenberg', 'bible-kjv.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README                   burgess-busterbrown.txt  milton-paradise.txt\r\n",
      "austen-emma.txt          carroll-alice.txt        shakespeare-caesar.txt\r\n",
      "austen-persuasion.txt    chesterton-ball.txt      shakespeare-hamlet.txt\r\n",
      "austen-sense.txt         chesterton-brown.txt     shakespeare-macbeth.txt\r\n",
      "bible-kjv.txt            chesterton-thursday.txt  whitman-leaves.txt\r\n",
      "blake-poems.txt          edgeworth-parents.txt\r\n",
      "bryant-stories.txt       melville-moby_dick.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel ngrams across several books\n",
      "CPU times: user 1.81 s, sys: 88.3 ms, total: 1.9 s\n",
      "Wall time: 2.85 s\n",
      "\n",
      "     a great deal 175\n",
      "       i dare say 107\n",
      "farmer browns boy 88\n",
      "  the sperm whale 86\n",
      "    the same time 84\n",
      "      i dont know 76\n",
      "     two or three 74\n",
      "    a few minutes 73\n",
      "  the white whale 71\n",
      "       mr and mrs 71\n",
      "\n",
      "   at the same time 76\n",
      "    a great deal of 66\n",
      " for the first time 48\n",
      "     in a low voice 36\n",
      "   i should like to 36\n",
      "    out of the room 34\n",
      " of the sperm whale 31\n",
      "much obliged to you 29\n",
      "  i beg your pardon 28\n",
      " at the same moment 26\n"
     ]
    }
   ],
   "source": [
    "print(\"Parallel ngrams across several books\")\n",
    "%time pcounts = ngrams_parallel(view, gutenberg_files, 3)\n",
    "print()\n",
    "print_common(pcounts, 10)\n",
    "pcounts = ngrams_parallel(view, gutenberg_files, 4)\n",
    "print()\n",
    "print_common(pcounts, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
